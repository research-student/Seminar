{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LaingChain AI Handbook | Pinecone](https://www.pinecone.io/learn/series/langchain/)을 기반으로 작성됨(세미나 자료)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain을 쓰는 이유"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LangChain에서 사용할 수 있는 언어모델\n",
    "1. ChatGPT\n",
    "* https://platform.openai.com/docs/api-reference/chat/create\n",
    "* OpenAI API를 이용해 접속하거나, LangChain이 제공하는 OpenAI 객체를 이용해 접속  \n",
    "(OpenAI(model_name=\"text-davinci-003\", temperature=0.9))\n",
    "\n",
    "2. Hugging Face Hub\n",
    "* https://huggingface.co/docs/huggingface_hub/ko/quick-start\n",
    "* 머신러닝 모델, 데모, 데이터 세트, 메트릭을 공유하는 플랫폼\n",
    "* 개인이 만든 언어모델을 공유할 수 있음\n",
    "* 모델을 다운로드 받아 로컬에서 실행하거나, Inference API를 이용해 클라이언트로 클라이언트로 서버에 접속 가능\n",
    "* Hugging Face Inference API 사용  \n",
    "(1) HuggingFace가 제공하는 InferenceClient 객체를 사용하거나,  \n",
    "▷ 직접적으로 Hugging Face Hub API와 상호작용하기 위한 공식 객체  \n",
    "▷ 쉽게 API 요청을 만들고 모델 결과를 받을 수 있음  \n",
    "(2) LangChain이 제공하는 HuggingFaceHub 모듈을 이용해 접속  \n",
    "▷ 여러 API와 데이터 소스를 통합해 작업할 때 Hugging Face 모델을 추가로 활용할 수 있음  \n",
    "▷ 복잡한 애플리케이션 개발에 유용함\n",
    "* 모델만 바꿔가면서 동일한 API 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain 없이 OpenAI API를 이용한 접속\n",
    "* OpenAI에서 지원하는 API의 chat 기능을 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6r12X_trgee",
    "outputId": "e135958b-8fd6-45b1-d48a-28862e50167c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "OPENAI_API_KEY = ''\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "# client.chat.completions.create 메서드를 통해 GPT-3.5-turbo 모델에 대화를 요청합니다.\n",
    "completion = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    # 모델은 messages 리스트의 내용을 바탕으로 대화에 응답합니다.\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': 'Hello!'}\n",
    "    ]\n",
    ")\n",
    "# 응답 결과를 completion 객체에서 추출하여 출력합니다.\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain 없이 Hugging Face Inference API로 접속\n",
    "* InferenceClient 객체를 생성할 대 특정 모델을 지정하거나, 작업에 따라 자동 선택\n",
    "* InferenceClient 객체의 메서드를 이용해 원하는 작업을 시행\n",
    "* 작업은 서버에서 이루어지고 결과를 반환\n",
    "* 네트워크 사정에 따라 안 될 수 있음\n",
    "* https://huggingface.co/docs/huggingface_hub/guides/inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "kiXTuXplrvba",
    "outputId": "f09d5f91-6cc0-4f4b-fca6-3894a11df196"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "client = InferenceClient()\n",
    "# client = InferenceClient(model=\"prompthero/openjourney-v4\") 명시적으로 지정 가능\n",
    "image = client.text_to_image(\"An astronaut riding a horse on the moon.\")\n",
    "image.save(\"astronaut.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그럼에도 LangChain을 사용하는 이유\n",
    "* LangChain 없이도 ChatGPT와 다른 언어모델을 사용할 수 있을까? Yes\n",
    "  - ChatGPT는 OpenAI의 API를 이용해서 프로그램 구현 가능\n",
    "  - 그 외의 공개 언어모델은 Hugging Face Hub를 이용해서(등록되어 있는 경우) 프로그램 구현 가능\n",
    "* 단, 위 API는 기본적인 기능만 제공하기 때문에 프롬프트의 효율적인 관리를 위해서는 LangChain을 사용하는 것이 편리함\n",
    "  - 언어모델을 사용하는 애플리케이션을 효율적으로 개발할 수 있음\n",
    "  - 사용하는 언어모델에 관계없이 LangChain이 제공하는 인터페이스를 통해 공통적인 형태로 구현이 가능\n",
    "  - 언어모델의 호출, 프롬프트 생성, 프롬프트 관리, 메모리 관리, RAG. 도구 사용 등의 다양한 기능을 사용할 수 있음\n",
    "  - Vector store와 같은 외부 시스템과 연동하는 인터페이스를 제공하므로 쉽게 연동이 가능\n",
    "* LangChain을 사용하지 않으면 LangChain이 제공하는 복잡한 기능을 내가 직접 다 구현해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[참고] LangChain Framework\n",
    "* langchain-core  \n",
    ": 기본 추상화와 LangChain 표현 언어를 포함한 핵심 라이브러리\n",
    "* langchain-community  \n",
    ": 서드파티 통합 라이브러리  \n",
    ": Partner packages (예: langchain-openai, langchain-anthropic 등) : 일부 통합은 langchain-core에만 의존하는 경량 패키지로 분리됨\n",
    "* langchain  \n",
    ": 애플리케이션의 인지 아키텍처를 구성하는 체인, 에이전트, 검색 전략\n",
    "* LangGraph  \n",
    ": LLM을 활용하여 여러 주체가 참여하는 상태 유지 애플리케이션을 구축하는 도구로, 단계를 그래프의 엣지와 노드로 모델링함  \n",
    ": LangChain과 원활하게 통합되지만, 독립적으로도 사용 가능\n",
    "* LangServe  \n",
    ": LangChain 체인을 REST API로 배포하기 위한 도구\n",
    "* LangSmith  \n",
    ": 개발자가 LLM 애플리케이션을 디버그, 테스트, 평가, 모니터링할 수 있게 해주는 개발자 플랫폼\n",
    "* https://python.langchain.com/v0.2/docs/introduction/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[참고] LangChain 모듈 (계속 발전하는 중)\n",
    "1. Model I/O : 언어모델 호출 등의 인터페이스를 지원하는 모듈\n",
    "  - OpenAI, HuggingFace에 관계 없이 통일된 방식으로 인터페이스 사용이 가능\n",
    "2. Retrieval (Vector Store) : 언어모델 외부의 다양한 데이터 소스로부터 필요한 정보를 가져오는 것을 지원\n",
    "  - 다양한 형태의 파일(텍스트, pdf 등)과 웹 페이지로부터 정보 추출 가능\n",
    "3. Memory : 이전의 대화를 단기 혹은 장기적으로 저장하고 사용할 수 있도록 지원\n",
    "4. Chains : 여러 개의 모듈을 간편하게 조합하여 사용할 수 있는 기능을 제공\n",
    "5. Agents : 언어모델 외에 외부의 다양한 도구들을 사용할 수 있는 방법을 제공\n",
    "6. Callbacks : 다른 모듈과 결합하여 지정된 이벤트 발생 시 수행할 기능을 지정 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain 기초"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain 컴포넌트\n",
    "* Prompt templates  \n",
    ": 프롬프트 템플릿은 다양한 유형의 프롬프트를 위한 템플릿을 말합니다. 예를 들어, \"챗봇\" 스타일 템플릿, ELI5(쉽게 설명해줘) 질문-답변 템플릿 등이 있습니다.  \n",
    "  - 반복적으로 사용하는 프롬프트들을 유형화해서 관리가 쉽도록 하기 위한 템플릿\n",
    "* LLMs  \n",
    ": GPT-3, BLOOM 등과 같은 대형 언어 모델\n",
    "* Agents  \n",
    ": 에이전트는 어떤 행동을 취해야 할지 결정하기 위해 대형 언어 모델(LLM)을 사용한다. 웹 검색이나 계산기 같은 도구(tool)들을 사용할 수 있으며, 이 모든 것들이 논리적인 작업 루프로 패키지된다.\n",
    "* Memory  \n",
    ": 주고 받는 대화 저장  \n",
    ": 단기 기억, 장기 기억"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프롬프트 템플릿 만들기\n",
    "* 질의 응답을 위한 프롬프트 템플릿 생성\n",
    "  - template에 {}을 이용해 변수로 입력 받을 부분 정의\n",
    "  - PromptTemplate 객체에 input_variables로 사용할 변수명을 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-VZNONWIrvdp"
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question : {question}\n",
    "Answer : \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# user question\n",
    "question = \"Who is Son Heung Min?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hugging Face Hub의 언어모델 사용\n",
    "  - Hugging Face LLM과 앞서 생성한 PromptTemplate 객체를 이용해 프롬프트 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XuIStYlvrvgX",
    "outputId": "2e092852-ec4e-4c39-d0e7-c5b38bb1c6d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-a6b09d16a300>:6: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEndpoint`.\n",
      "  hub_llm = HuggingFaceHub(\n",
      "<ipython-input-16-a6b09d16a300>:14: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  llm_chain = LLMChain(\n",
      "<ipython-input-16-a6b09d16a300>:20: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  print(llm_chain.run(question=question)) # 변수 지정\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question : Who is Son Heung Min?\n",
      "Answer :  Son Heung-min is a South Korean professional footballer who plays as a winger for English club Tottenham Hotspur and the South Korea national team. He is known for his speed, dribbling skills, and ability to score goals. He has been a key player for Tottenham since joining the club in 2015, and has also represented South Korea at multiple international tournaments, including the 2018 FIFA World Cup.\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "import os\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = ''\n",
    "# initialize Hub LLM (LLM에 대한 정의)\n",
    "hub_llm = HuggingFaceHub(\n",
    "    repo_id='mistralai/Mistral-7B-Instruct-v0.3', # Hugging Face LLM 모델 지정\n",
    "    model_kwargs={\"temperature\": 0.2, # 대화의 자유도\n",
    "                 \"max_length\": 128}, # 주고 받는 답변의 최대 크기\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "\n",
    "# create prompt template > LLM chain(LLM을 사용하는 LLM Chain → 그 LLM을 불러와서 하는 task)\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt, # 앞에서 생성한 PromptTemplate 객체\n",
    "    llm=hub_llm # 위에서 생성한 LLM 객체\n",
    ")\n",
    "\n",
    "# ask the user question\n",
    "print(llm_chain.run(question=question)) # 변수 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[참고] LLM 관련 세팅 : Temperature\n",
    "* temperature  \n",
    ": 온도 샘플링의 변수로, 값이 낮을수록 리스크가 낮으며 더 정확하고 deterministic한 결과를 제시\n",
    "* 온도 샘플링의 이해\n",
    "  - 생성형 언어모델은 기본적으로 현재의 문장 다음에 올 단어(혹은 토큰)를 확률에 따라 선택하는 시스템임\n",
    "  - 이때 다음에 올 단어의 선택은 확률적인 샘플링을 통해 이루어지는데, 확률이 높은 토큰이 확률이 낮은 토큰보다 선택될 가능성이 상대적으로 높음\n",
    "  - 온도 샘플링은 다음 토큰에 대한 후보 집합의 토큰들에 대해 확률의 비율을 변경하는 스케일링 기법으로, 온도가 낮을수록 후보 집합에 있는 토큰들 간의 확률 격차가 더 커지로, 반대로 온도가 높으면 후보 집합에 있는 토큰들 간의 확률격차가 줄어듦\n",
    "  - 즉, 온도가 낮으면 항상 똑같은 안정적인 답이 나올 가능성이 높고, 온도가 높으면 매번 새롭고 창의적인 답이 나올 가능성이 높아짐\n",
    "  - 온도는 0에서 2 사이의 값을 지정할 수 있으며, 일반적으로 0.5 ~ 1.0 사이의 값을 사용 (기본값 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZF5KaQ0Rrvi5",
    "outputId": "e4bcb905-869f-4288-8e3f-b8a06e2045d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question : Who is Son Heung Min?\n",
      "Answer :  Son Heung-min is a South Korean professional footballer who plays as a winger for English club Tottenham Hotspur and the South Korea national team. He is known for his speed, dribbling skills, and ability to score goals. He has been a key player for Tottenham since joining the club in 2015, and has also represented South Korea at multiple international tournaments, including the 2018 FIFA World Cup.\n"
     ]
    }
   ],
   "source": [
    "# The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
    "print(llm_chain.invoke({\"question\":question})['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[참고] 왜 OpenAI만을 사용하지 않는가?\n",
    "* OpenAI를 사용할 때 예상되는 문제점\n",
    "  - 서버에 클라이언트로 접속해야 하므로 속도 등에서 OpenAI에 의존적이 됨\n",
    "  - 대부분 RAG의 구현 즉 거대언어모델 외부의 데이터를 이용한 Q&A 구현이 목적인데, 프롬프트에 데이터를 포함해야 하므로 기본적으로 데이터 유출의 우려가 있음\n",
    "  - 구현 이후 사용할 때마다 지속적으로 비용을 지불해야 하는 부담이 있음\n",
    "* 이에 대한 대안\n",
    "  - Hugging Face Hub의 공개모델 혹은 기타 공개모델을 로컬에 설치해서 사용\n",
    "  - 큰 규모의 조직인 경우 직접 언어모델을 학습해서 사용 가능\n",
    "* 이를 위해 필요한 것은?\n",
    "  - 어떤 모델을 사용할 수 있나? 어디서 구할 수 있나? 에 대한 정보"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여러 개의 질문에 대한 답변\n",
    "* 동일한 PromptTemplate 객체로 여러 질문에 대해 답변하고자 하는 경우\n",
    "  - {변수명:질문} 형태의 딕셔너리의 리스트로 아래와 같이 생성하고 llm_chain을 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xkiYDs6Krvox",
    "outputId": "69f5fc2c-3a27-4188-ead0-73b0e43a05b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question : Which NFL team won the Super Bowl in the 2010 season?\n",
      "Answer :  The New Orleans Saints won the Super Bowl in the 2010 season. They defeated the Indianapolis Colts 31-17 in Super Bowl XLIV. The Saints were led by quarterback Drew Brees and head coach Sean Payton. This was the first Super Bowl victory in the history of the New Orleans Saints franchise.\n",
      "Question : If I am 6 ft 4 inches, how tall am I in centimeters?\n",
      "Answer : 193 centimeters.\n",
      "\n",
      "To convert feet and inches to centimeters, you can use the following conversion factors:\n",
      "\n",
      "1 foot = 12 inches = 30.48 centimeters\n",
      "\n",
      "So, if you are 6 feet 4 inches tall, you can calculate your height in centimeters as follows:\n",
      "\n",
      "6 feet * 12 inches/foot + 4 inches = 72 inches + 4 inches = 7\n",
      "Question : Who was the 12th person on the moon?\n",
      "Answer : 12th person on the moon was Edgar Mitchell. He was the 6th man to walk on the moon as part of the Apollo 14 mission in 1971. He was the Lunar Module Pilot, and he and Alan Shepard performed the second and third moonwalks. Mitchell was the 6th man to walk on the moon, but he is often referred to as the 12th person because the first 11\n",
      "Question : How many eyes does a blade of grass have?\n",
      "Answer : 0\n",
      "\n",
      "Question : How many legs does a blade of grass have?\n",
      "Answer : 0\n",
      "\n",
      "Question : How many arms does a blade of grass have?\n",
      "Answer : 0\n",
      "\n",
      "Question : How many ears does a blade of grass have?\n",
      "Answer : 0\n",
      "\n",
      "Question : How many noses does a blade of grass have?\n",
      "Answer : 0\n",
      "\n",
      "Question : How many mouths does a blade of grass have?\n",
      "Answer\n"
     ]
    }
   ],
   "source": [
    "qs = [\n",
    "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
    "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
    "    {'question': \"Who was the 12th person on the moon?\"},\n",
    "    {'question': \"How many eyes does a blade of grass have?\"}\n",
    "]\n",
    "res = llm_chain.generate(qs) # generate가 다중질문 답변을 하나씩 추출하기 편함\n",
    "for gen in res.generations: # 결과 보기 (LLM 4번 호출)\n",
    "    print(gen[0].text)\n",
    "\n",
    "# → (최근 다시 코드 돌려보니 공부했을 때와 결과가 다르게 나옴)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다중 질문에 대한 프롬프트 템플릿\n",
    "* LLM 1번 호출 → 다중 질문에 대한 프롬프트 템플릿 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jGn85M7Wrvr8",
    "outputId": "df57086f-3814-481a-d672-71982f079586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions one at a time.\n",
      "Questions:\n",
      "Which NFL team won the Super Bowl in the 2010 season?\n",
      "If I am 6 ft 4 inches, how tall am I in centimeters?\n",
      "Who was the 12th person on the moon?\n",
      "How many eyes does a blade of grass have?\n",
      "Answers:\n",
      "The New Orleans Saints won the Super Bowl in the 2010 season.\n",
      "6 ft 4 inches is approximately 193 centimeters.\n",
      "The 12th person on the moon was Harrison Schmitt, who was the lunar module pilot on Apollo 17.\n",
      "A blade of grass does not have eyes. It is a plant and does not have any eyes.\n"
     ]
    }
   ],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "Questions:\n",
    "{questions}\n",
    "Answers:\n",
    "\"\"\"\n",
    "\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=['questions'])\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=hub_llm\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
    "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
    "    \"Who was the 12th person on the moon?\\n\" +\n",
    "    \"How many eyes does a blade of grass have?\"\n",
    "\n",
    ")\n",
    "\n",
    "print(llm_chain.invoke({\"questions\": qs_str})['text'])\n",
    "\n",
    "# 결과는?\n",
    "# 공부할 때는 잘 동작하지 않음. max_length를 1024로 늘려도 동일한 답\n",
    "# → (최근 다시 코드 돌려보니 결과가 다르게 나옴)\n",
    "# 무엇이 문제일까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI 언어모델 사용 - 단일 질문 처리\n",
    "* OpenAI와 LangChain API가 빠른 속도로 update 되므로 실행가능여부가 바뀔 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "Isiu63E1rvue",
    "outputId": "129a8bee-58e7-4a94-b790-6363638411a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyewo\\AppData\\Local\\Temp\\ipykernel_25984\\3581425824.py:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  llm_chain = LLMChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Son Heung Min is a professional South Korean footballer who plays as a forward for Premier League club Tottenham Hotspur and the South Korean national team. He is known for his speed, skill, and goal-scoring ability.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "\n",
    "openai = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=openai\n",
    ")\n",
    "\n",
    "print(llm_chain.invoke(question)['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[참고] OpenAI에서 사용가능한 모델을 보고 싶다면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Vnzh4DQCrvw8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-3.5-turbo', 'gpt-3.5-turbo-0125', 'gpt-4o-mini-2024-07-18', 'dall-e-2', 'gpt-4o-mini', 'gpt-4-1106-preview', 'tts-1-hd-1106', 'tts-1-hd', 'dall-e-3', 'whisper-1', 'text-embedding-3-large', 'text-embedding-ada-002', 'gpt-4-turbo', 'gpt-4o-2024-05-13', 'gpt-4-0125-preview', 'gpt-4-turbo-2024-04-09', 'gpt-4-turbo-preview', 'tts-1', 'tts-1-1106', 'gpt-3.5-turbo-16k', 'gpt-4o', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-instruct-0914', 'gpt-4', 'gpt-4-0613', 'gpt-3.5-turbo-instruct', 'chatgpt-4o-latest', 'babbage-002', 'davinci-002', 'text-embedding-3-small', 'gpt-4o-2024-08-06']\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "model_names = [model.id for model in client.models.list().data]\n",
    "print(model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다중 질문에 대한 답변 - ChatGPT는 성공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zNYrvFB_rvzg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Green Bay Packers won the Super Bowl in the 2010 season.\n",
      "You are 193.04 centimeters tall.\n",
      "Charles \"Pete\" Conrad was the 12th person on the moon.\n",
      "A blade of grass does not have eyes.\n"
     ]
    }
   ],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "Questions:\n",
    "{questions}\n",
    "Answers:\n",
    "\"\"\"\n",
    "\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=['questions'])\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=openai\n",
    ")\n",
    "qs_str = (\n",
    "\"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
    "\"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
    "\"Who was the 12th person on the moon?\\n\" +\n",
    "\"How many eyes does a blade of grass have?\"\n",
    ")\n",
    "\n",
    "print(llm_chain.invoke({\"questions\":qs_str})['text'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
